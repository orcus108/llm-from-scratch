# Chapter 1: Working with Text Data

## Main Chapter Code

- [01_main-code](01_main-code) contains the main chapter code.

## Bonus Materials

- [02_efficient_mha_implementations](02_efficient_mha_implementations) contains code to compare the efficiency of different implementations of Multi-Head Attention (MHA) [work in progress]
- [03_understanding-buffers](03_understanding-buffers) is an intuitive explanation of buffers and why they're used
- [04_rotary-position-embedding](04_rotary-position-embedding) contains implementation of Rotary Position Embeddings (RoPE) instead of the naive absolute position embedding [need to prettify]