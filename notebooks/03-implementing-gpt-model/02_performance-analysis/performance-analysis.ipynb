{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c27cd9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thop version: 0.1.1-2209072238\n",
      "torch version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"thop\", \n",
    "    \"torch\"\n",
    "]\n",
    "\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29831581",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from thop import profile \n",
    "\n",
    "from modules import GPTModel\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024,\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84b1d1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"[INFO] Using {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "input_tensor = torch.randint(0, 50257, (batch_size, 1024)).to(device) # (batch_size, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac59a69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt-small (124M)  : 5.1e+11 FLOPS\n",
      "gpt-medium (355M) : 1.4e+12 FLOPS\n",
      "gpt-large (774M)  : 3.2e+12 FLOPS\n",
      "gpt-xl (1558M)    : 6.4e+12 FLOPS\n"
     ]
    }
   ],
   "source": [
    "for size in model_configs:\n",
    "    cfg = {**BASE_CONFIG, **model_configs[size]}\n",
    "\n",
    "    model = GPTModel(cfg).bfloat16()\n",
    "    model.to(device)\n",
    "\n",
    "    # MACS = multiply-accumulate operations\n",
    "    # MACS are typically counted as two FLOPS (one multiply and one accumulate)\n",
    "    macs, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "    flops = 2 * macs\n",
    "    print(f\"{size:18}: {flops:.1e} FLOPS\")\n",
    "\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d77fbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252993601536.0, 123614976.0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {**BASE_CONFIG, **model_configs[\"gpt-small (124M)\"]}\n",
    "model = GPTModel(cfg).bfloat16().to(device)\n",
    "\n",
    "profile(model, inputs=(input_tensor,), verbose=False) # returns (macs, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1609fec8",
   "metadata": {},
   "source": [
    "Now, we can use these measured FLOPS to estimate training FLOPS and the training time and cost on some popular chips."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3641631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "# forward pass model FLOPS\n",
    "FORWARD_FLOPS = {\n",
    "    \"gpt-small (124M)\": 5.1e11,\n",
    "    \"gpt-medium (355M)\": 1.4e12,\n",
    "    \"gpt-large (774M)\": 3.2e12,\n",
    "    \"gpt-xl (1558M)\": 6.4e12,\n",
    "}\n",
    "\n",
    "# hardware profiles - sustained TFLOPS + $/hour\n",
    "HARDWARE = {\n",
    "    \"CPU\":        {\"tflops\": 0.3,  \"cost_per_hour\": 0.05},\n",
    "    \"M1\":         {\"tflops\": 2.6,  \"cost_per_hour\": 0.00},  # my device\n",
    "    \"RTX 3090\":   {\"tflops\": 30.0, \"cost_per_hour\": 1.00},\n",
    "    \"RTX 4090\":   {\"tflops\": 60.0, \"cost_per_hour\": 1.50},\n",
    "    \"A100 80GB\":  {\"tflops\": 250.0,\"cost_per_hour\": 2.50},\n",
    "    \"H100\":       {\"tflops\": 500.0,\"cost_per_hour\": 4.50},\n",
    "}\n",
    "\n",
    "# training set up\n",
    "NUM_TOKENS = 10_000_000\n",
    "CONTEXT_LENGTH = 1024\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "TRAINING_MULTIPLIER = 3         # training = forward + backward + update = 3 * forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f17ac7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_steps(num_tokens, context_length, batch_size):\n",
    "    return num_tokens // (context_length * batch_size)\n",
    "\n",
    "def total_training_flops(forward_flops, steps):\n",
    "    return TRAINING_MULTIPLIER * forward_flops * steps\n",
    "\n",
    "def flops_to_time_seconds(total_flops, tflops):\n",
    "    return total_flops / (tflops * 1e12)\n",
    "\n",
    "def time_to_cost(hours, cost_per_hour):\n",
    "    return hours * cost_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad0b6338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset tokens      : 10,000,000\n",
      "Context length      : 1024\n",
      "Batch size          : 2\n",
      "Training steps      : 4,882\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-small (124M)\n",
      "Total training FLOPs: 7.47e+15\n",
      "  CPU        |    6.916 hrs | $  0.35\n",
      "  M1         |    0.798 hrs | $  0.00\n",
      "  RTX 3090   |    0.069 hrs | $  0.07\n",
      "  RTX 4090   |    0.035 hrs | $  0.05\n",
      "  A100 80GB  |    0.008 hrs | $  0.02\n",
      "  H100       |    0.004 hrs | $  0.02\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-medium (355M)\n",
      "Total training FLOPs: 2.05e+16\n",
      "  CPU        |   18.986 hrs | $  0.95\n",
      "  M1         |    2.191 hrs | $  0.00\n",
      "  RTX 3090   |    0.190 hrs | $  0.19\n",
      "  RTX 4090   |    0.095 hrs | $  0.14\n",
      "  A100 80GB  |    0.023 hrs | $  0.06\n",
      "  H100       |    0.011 hrs | $  0.05\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-large (774M)\n",
      "Total training FLOPs: 4.69e+16\n",
      "  CPU        |   43.396 hrs | $  2.17\n",
      "  M1         |    5.007 hrs | $  0.00\n",
      "  RTX 3090   |    0.434 hrs | $  0.43\n",
      "  RTX 4090   |    0.217 hrs | $  0.33\n",
      "  A100 80GB  |    0.052 hrs | $  0.13\n",
      "  H100       |    0.026 hrs | $  0.12\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-xl (1558M)\n",
      "Total training FLOPs: 9.37e+16\n",
      "  CPU        |   86.791 hrs | $  4.34\n",
      "  M1         |   10.014 hrs | $  0.00\n",
      "  RTX 3090   |    0.868 hrs | $  0.87\n",
      "  RTX 4090   |    0.434 hrs | $  0.65\n",
      "  A100 80GB  |    0.104 hrs | $  0.26\n",
      "  H100       |    0.052 hrs | $  0.23\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "steps = training_steps(NUM_TOKENS, CONTEXT_LENGTH, BATCH_SIZE)\n",
    "\n",
    "print(f\"\\nDataset tokens      : {NUM_TOKENS:,}\")\n",
    "print(f\"Context length      : {CONTEXT_LENGTH}\")\n",
    "print(f\"Batch size          : {BATCH_SIZE}\")\n",
    "print(f\"Training steps      : {steps:,}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for model_name, fwd_flops in FORWARD_FLOPS.items():\n",
    "    total_flops = total_training_flops(fwd_flops, steps)\n",
    "\n",
    "    print(f\"\\nMODEL: {model_name}\")\n",
    "    print(f\"Total training FLOPs: {total_flops:.2e}\")\n",
    "\n",
    "    for hw_name, hw in HARDWARE.items():\n",
    "        seconds = flops_to_time_seconds(\n",
    "            total_flops, hw[\"tflops\"]\n",
    "        )\n",
    "\n",
    "        hours = seconds / 3600\n",
    "        cost = time_to_cost(hours, hw[\"cost_per_hour\"])\n",
    "\n",
    "        print(\n",
    "            f\"  {hw_name:10} | \"\n",
    "            f\"{hours:8.3f} hrs | \"\n",
    "            f\"${cost:6.2f}\"\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "008de91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference set up\n",
    "CONTEXT_LENGTH = 1024\n",
    "BATCH_SIZE = 1              # typical inference\n",
    "GENERATED_TOKENS = 200      # per request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d6b0499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_flops_per_request(\n",
    "    forward_flops,\n",
    "    context_length,\n",
    "    generated_tokens,\n",
    "    batch_size=1\n",
    "):\n",
    "    flops_per_token = forward_flops / context_length\n",
    "    return flops_per_token * generated_tokens * batch_size\n",
    "\n",
    "\n",
    "def flops_to_time_seconds(total_flops, tflops):\n",
    "    return total_flops / (tflops * 1e12)\n",
    "\n",
    "\n",
    "def time_to_cost(hours, cost_per_hour):\n",
    "    return hours * cost_per_hour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db4da04e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inference setup:\n",
      "Context length     : 1024\n",
      "Generated tokens   : 200\n",
      "Batch size         : 1\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-small (124M)\n",
      "Inference FLOPs per request: 9.96e+10\n",
      "  CPU        |  332.03 ms |    602.4 tok/s | $0.000005 per req | $0.0231 per 1M tok\n",
      "  M1         |   38.31 ms |   5220.4 tok/s | $0.000000 per req | $0.0000 per 1M tok\n",
      "  RTX 3090   |    3.32 ms |  60235.3 tok/s | $0.000001 per req | $0.0046 per 1M tok\n",
      "  RTX 4090   |    1.66 ms | 120470.6 tok/s | $0.000001 per req | $0.0035 per 1M tok\n",
      "  A100 80GB  |    0.40 ms | 501960.8 tok/s | $0.000000 per req | $0.0014 per 1M tok\n",
      "  H100       |    0.20 ms | 1003921.6 tok/s | $0.000000 per req | $0.0012 per 1M tok\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-medium (355M)\n",
      "Inference FLOPs per request: 2.73e+11\n",
      "  CPU        |  911.46 ms |    219.4 tok/s | $0.000013 per req | $0.0633 per 1M tok\n",
      "  M1         |  105.17 ms |   1901.7 tok/s | $0.000000 per req | $0.0000 per 1M tok\n",
      "  RTX 3090   |    9.11 ms |  21942.9 tok/s | $0.000003 per req | $0.0127 per 1M tok\n",
      "  RTX 4090   |    4.56 ms |  43885.7 tok/s | $0.000002 per req | $0.0095 per 1M tok\n",
      "  A100 80GB  |    1.09 ms | 182857.1 tok/s | $0.000001 per req | $0.0038 per 1M tok\n",
      "  H100       |    0.55 ms | 365714.3 tok/s | $0.000001 per req | $0.0034 per 1M tok\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-large (774M)\n",
      "Inference FLOPs per request: 6.25e+11\n",
      "  CPU        | 2083.33 ms |     96.0 tok/s | $0.000029 per req | $0.1447 per 1M tok\n",
      "  M1         |  240.38 ms |    832.0 tok/s | $0.000000 per req | $0.0000 per 1M tok\n",
      "  RTX 3090   |   20.83 ms |   9600.0 tok/s | $0.000006 per req | $0.0289 per 1M tok\n",
      "  RTX 4090   |   10.42 ms |  19200.0 tok/s | $0.000004 per req | $0.0217 per 1M tok\n",
      "  A100 80GB  |    2.50 ms |  80000.0 tok/s | $0.000002 per req | $0.0087 per 1M tok\n",
      "  H100       |    1.25 ms | 160000.0 tok/s | $0.000002 per req | $0.0078 per 1M tok\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "MODEL: gpt-xl (1558M)\n",
      "Inference FLOPs per request: 1.25e+12\n",
      "  CPU        | 4166.67 ms |     48.0 tok/s | $0.000058 per req | $0.2894 per 1M tok\n",
      "  M1         |  480.77 ms |    416.0 tok/s | $0.000000 per req | $0.0000 per 1M tok\n",
      "  RTX 3090   |   41.67 ms |   4800.0 tok/s | $0.000012 per req | $0.0579 per 1M tok\n",
      "  RTX 4090   |   20.83 ms |   9600.0 tok/s | $0.000009 per req | $0.0434 per 1M tok\n",
      "  A100 80GB  |    5.00 ms |  40000.0 tok/s | $0.000003 per req | $0.0174 per 1M tok\n",
      "  H100       |    2.50 ms |  80000.0 tok/s | $0.000003 per req | $0.0156 per 1M tok\n",
      "--------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nInference setup:\")\n",
    "print(f\"Context length     : {CONTEXT_LENGTH}\")\n",
    "print(f\"Generated tokens   : {GENERATED_TOKENS}\")\n",
    "print(f\"Batch size         : {BATCH_SIZE}\")\n",
    "print(\"-\" * 110)\n",
    "\n",
    "for model_name, fwd_flops in FORWARD_FLOPS.items():\n",
    "    inf_flops = inference_flops_per_request(\n",
    "        fwd_flops,\n",
    "        CONTEXT_LENGTH,\n",
    "        GENERATED_TOKENS,\n",
    "        BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    print(f\"\\nMODEL: {model_name}\")\n",
    "    print(f\"Inference FLOPs per request: {inf_flops:.2e}\")\n",
    "\n",
    "    for hw_name, hw in HARDWARE.items():\n",
    "        seconds = flops_to_time_seconds(inf_flops, hw[\"tflops\"])\n",
    "        hours = seconds / 3600\n",
    "        cost = time_to_cost(hours, hw[\"cost_per_hour\"])\n",
    "\n",
    "        tokens_per_sec = GENERATED_TOKENS / seconds if seconds > 0 else float(\"inf\")\n",
    "        cost_per_million = cost * (1_000_000 / GENERATED_TOKENS)\n",
    "\n",
    "        print(\n",
    "            f\"  {hw_name:10} | \"\n",
    "            f\"{seconds*1000:7.2f} ms | \"\n",
    "            f\"{tokens_per_sec:8.1f} tok/s | \"\n",
    "            f\"${cost:.6f} per req | \"\n",
    "            f\"${cost_per_million:.4f} per 1M tok\"\n",
    "        )\n",
    "\n",
    "    print(\"-\" * 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58819caf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
