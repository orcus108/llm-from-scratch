{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bde226a",
   "metadata": {},
   "source": [
    "# **Multi-Head Latent Attention (MLA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda4e13c",
   "metadata": {},
   "source": [
    "inspired by the [huggingface implementation](https://huggingface.co/bird-of-paradise/deepseek-mla) of MLA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef98f7b9",
   "metadata": {},
   "source": [
    "MLA module as in DeepSeekV2 paper.\n",
    "\n",
    "Their key innovations:\n",
    "1. Low-Rank Key-Value Joint Compression\n",
    "2. Decoupled Rotary Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d0139b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e465bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_freqs_cis(dim, end, theta=10000.0):\n",
    "    \"\"\"\n",
    "    Generates the complex-valued rotation factors used in RoPE for \n",
    "        each position and each pair of embedding dimensions.\n",
    "\n",
    "    Args: \n",
    "        dim (int): embedding dimension (must be even)\n",
    "        end (int): maximum sequence length to generate frequencies for\n",
    "        theta (float): base frequency used to compute inverse wavelengths\n",
    "    \"\"\"\n",
    "\n",
    "    i = torch.arange(dim // 2).float()                     # (dim // 2, )\n",
    "    freqs = 1.0 / (theta ** (2 * i / dim))                 # (dim // 2, )\n",
    "    t = torch.arange(end, device=freqs.device)             # (end, )\n",
    "    freqs = torch.outer(t, freqs).float()                  # (end, dim // 2)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # (end, dim // 2)\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c94db7",
   "metadata": {},
   "source": [
    "A note on how `torch.polar()` works:\n",
    "- `torch.polar(abs, angle)` returns a complex number of the form $\\text{abs}\\cdot \\exp(i \\cdot \\text{angle})$\n",
    "- `abs` is the magnitude of the complex number and `angle` is the angle this number makes with the positive real axis on a complex plan.\n",
    "- A refresher: $e^{i \\cdot \\theta} = \\cos{\\theta} + i \\cdot \\sin{\\theta}$\n",
    "- For example: `abs = 1` and `angle=0` <br>\n",
    "        then `torch.polar(1, 0)` = \n",
    "        $$\n",
    "            1 \\cdot e^{i \\cdot 0} = 1 \\cdot(\\cos{0} + i \\cdot \\sin{0}) \\\\\n",
    "            = 1 \\cdot (1 + 0 \\cdot i) \\\\\n",
    "            = 1 + 0 \\cdot i\n",
    "        $$\n",
    "- So, `torch.polar()` returns a complex number (which by default is of `dtype=complex64`).\n",
    "- However, note that this complex number still occupies only one positon in memory. <br>\n",
    "  So, the tensor returned by `torch.polar(abs, angle)` is of same shape as `angle` or `abs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_for_broadcast(freqs_cis, x):\n",
    "    \"\"\"\n",
    "    Reshapes the precomputed RoPE complex frequencies so they broadcast \n",
    "        correctly across the batch and head dimensions of the input tensor.\n",
    "\n",
    "    Args:\n",
    "        freqs_cis (torch.Tensor): complex RoPE frequencies of shape (seq_len, head_dim)\n",
    "        x (torch.Tensor): input tensor to be rotated, used to infer the broadcastable shape\n",
    "    \"\"\"\n",
    "\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd7c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(xq, xk, freqs_cis) -> Tuple:\n",
    "    \"\"\"\n",
    "    Applies rotary positional embeddings (RoPE) to query and key tensors by rotating \n",
    "        each pair of embedding dimensions using position-dependent complex phases.\n",
    "\n",
    "    Args:\n",
    "        xq (torch.Tensor): query tensor of shape (batch, seq_len, ..., head_dim)\n",
    "        xk (torch.Tensor): key tensor of shape (batch, seq_len, ..., head_dim)\n",
    "        freqs_cis (torch.Tensor): precomputed complex RoPE frequencies of shape (max_seq_len, head_dim).\n",
    "    \"\"\"\n",
    "\n",
    "    # validate input dimensions\n",
    "    assert xq.shape[-1] == xk.shape[-1], \"Query and Key must have same embedding dimension\" \n",
    "    assert xq.shape[-1] % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # get sequence length\n",
    "    q_len = xq.shape[1]\n",
    "    k_len = xk.shape[1]\n",
    "\n",
    "    # use appropriate part of freqs_cis for each sequence\n",
    "    q_freqs = freqs_cis[:q_len]\n",
    "    k_freqs = freqs_cis[:k_len]\n",
    "\n",
    "    # apply rotary embeddings separately\n",
    "    # split last dimension to [xq.shape[:-1]/2, 2]\n",
    "    xq_ = torch.view_as_complex(xq.float().rehsape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().rehsape(*xk.shape[:-1], -1, 2))\n",
    "\n",
    "    # reshape freqs for each\n",
    "    q_freqs = reshape_for_broadcast(q_freqs, xq_)\n",
    "    k_freqs = reshape_for_broadcast(k_freqs, xk_)\n",
    "\n",
    "    # works for both [batch_size, seq_len, n_heads * head_dim] and [batch_size, seq_len, n_heads, head_dim]\n",
    "    xq_out = torch.view_as_real(xq_ * q_freqs).flatten(xq.ndim-1)\n",
    "    xk_out = torch.view_as_real(xk_ * k_freqs).flatten(xk.ndim-1)\n",
    "\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    MLA (from the DeepSeek V2 paper)\n",
    "    \n",
    "    Args:\n",
    "        d_model:  total dimensions of the model.\n",
    "        num_head: number of attention heads.\n",
    "        d_embed:  embedding dimension\n",
    "        d_c:      K/V compression dimension\n",
    "        d_c1:     Q compression dimension\n",
    "        d_rotate: dimension for rotary position embedding\n",
    "        dropout:  dropout rate for attention scores\n",
    "        bias:     whether to include bias in linear projections\n",
    "\n",
    "        d_head:   inferred from d_model // num_head\n",
    "\n",
    "    Inputs:\n",
    "        sequence: input sequence for self-attention and the query for cross-attention\n",
    "        key_value_state: input for the key, values for cross-attention\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model,            # infer d_head from d_model\n",
    "        num_head,\n",
    "        d_embed,\n",
    "        d_c,\n",
    "        d_c1,\n",
    "        d_rotate,\n",
    "        dropout=0.1,\n",
    "        bias=True,\n",
    "        max_batch_size=32,  # for KV cache sizing\n",
    "        max_seq_len=2048    # for KV cache sizing\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        assert d_model % num_head == 0, \"d_model must be divisible by num_head\"\n",
    "        assert d_c < d_embed, \"Compression dim should be smaller than embedding dim\"\n",
    "        assert d_c1 < d_embed, \"Query compression dim should be smaller than embedding dim\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "        self.d_head = d_model // num_head\n",
    "        self.d_embed = d_embed\n",
    "\n",
    "        self.d_c = d_c\n",
    "        self.d_c1 = d_c1\n",
    "        self.d_rotate = d_rotate\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        # linear down-projection (compression) transformations\n",
    "        self.DKV_proj = nn.Linear(d_embed, d_c, bias=bias)\n",
    "        self.DQ_proj = nn.Linear(d_embed, d_c1, bias=bias)\n",
    "\n",
    "        # linear up-projection transformations\n",
    "        self.UQ_proj = nn.Linear(d_c1, d_model, bias=bias)\n",
    "        self.UK_proj = nn.Linear(d_c, d_model, bias=bias)\n",
    "        self.UV_proj = nn.Linear(d_c, d_model, bias=bias)\n",
    "\n",
    "        # linear RoPE-projection\n",
    "        self.RQ_proj = nn.Linear(d_c1, num_head * d_rotate, bias=bias)\n",
    "        self.RK_proj = nn.Linear(d_embed, d_rotate, bias=bias)\n",
    "\n",
    "        \n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
