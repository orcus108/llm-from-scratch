{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd2c3b6a",
   "metadata": {},
   "source": [
    "# **Grouped-Query Attention (GQA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049944a6",
   "metadata": {},
   "source": [
    "This notebook contains a simple implementation of the GQA mechanism. The goal is to show the core steps involved. \n",
    "In subsequent notebooks, this will be formally incorporated into the GPT architecture that we have implemented so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a8343a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c950caf",
   "metadata": {},
   "source": [
    "Let us set up some hyperparameters of our toy GPT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d8c2fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hparams\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 64\n",
    "\n",
    "n_q_heads = 8\n",
    "n_kv_heads = 2\n",
    "head_dim = d_model // n_q_heads "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51659d01",
   "metadata": {},
   "source": [
    "We must confirm that the number of KV heads is a factor of the number of Q heads (aka, `n_q_heads` is divisible by `n_kv_heads`) in order to succesfully group the KV heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da219157",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert n_q_heads % n_kv_heads == 0, \"Number of KV heads must be a factor of number of Q heads\"\n",
    "group_size = n_q_heads // n_kv_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa9dea40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Number of query heads: 8\n",
      "[INFO] Number of key-value heads: 2\n",
      "[INFO] Using group size: 4\n"
     ]
    }
   ],
   "source": [
    "print(f\"[INFO] Number of query heads: {n_q_heads}\")\n",
    "print(f\"[INFO] Number of key-value heads: {n_kv_heads}\")\n",
    "print(f\"[INFO] Using group size: {group_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db505eb",
   "metadata": {},
   "source": [
    "Now, we will use some random Q, K and V tensors.\n",
    "\n",
    "\n",
    "**Note**: assume these QKV tensors have been obtained after multiplying the input (x) with the Q, K and V matrices respectively.\n",
    "- The number of query weight matrices (W_query) will be equal to the number of query heads (which is the same as the number of attention heads since each head gets its own unique query matrix).\n",
    "- However, the number of key and value weight matrices (W_key and W_value) will be less than the number of attention heads. As each attention head doesn't get its own KV matrices. Instead, as per the GQA mechanism, the attention heads are grouped such that each group shares a common K and V matrix.\n",
    "- In this case, since there's 8 attention heads and the group size is 4. That means 4 attention heads share a common K and V weight matrix (W_key and W_value is the same for all heads in a group)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6088f2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 5, 8]) torch.Size([2, 2, 5, 8]) torch.Size([2, 2, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# random Q, K, V\n",
    "Q = torch.randn(batch_size, n_q_heads, seq_len, head_dim)\n",
    "K = torch.randn(batch_size, n_kv_heads, seq_len, head_dim)\n",
    "V = torch.randn(batch_size, n_kv_heads, seq_len, head_dim)\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe87339",
   "metadata": {},
   "source": [
    "Now, we apply the core GQA trick: expanding these K and V tensors such that each attention heads gets one for itself (but the heads in a group will get the same K, V tensors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1955365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 5, 8]) torch.Size([2, 8, 5, 8]) torch.Size([2, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# expand K, V to match Q heads (core GQA trick)\n",
    "K = K.repeat_interleave(group_size, dim=1)\n",
    "V = V.repeat_interleave(group_size, dim=1)\n",
    "# now: (batch, n_q_heads, seq_len, head_dim)\n",
    "print(Q.shape, K.shape, V.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6e1d1",
   "metadata": {},
   "source": [
    "The rest of the attention mechanism is the same as we say in MHA. We calculate the attention scores and then attention weights using the Q and K tensors and then get the final context vector (`output`) by taking a weighted sum of the V tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eadce209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "# scaled dot-product attention\n",
    "attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(head_dim)\n",
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "output = attn_weights @ V\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814a9c4c",
   "metadata": {},
   "source": [
    "- For the formal implementation of the GQA mechanism, checkout the [gpt_with_kv_gqa](gpt_with_kv_gqa.py) script.\n",
    "- The [README](README.md) file has the results of the comparison of the MHA and GQA mechanism."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
